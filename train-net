#!/usr/bin/python
# -*- coding: utf-8 -*-
import os
import sys
HERE = os.path.dirname(__file__)
sys.path.append(HERE)
import argparse
import charmodel
import random
import itertools
import re
import json
from language import load_texts, always, test_fn_search, training_fn_search
from language import read_truth_file
import mappings
import numpy as np
from pan import balance_results, write_answers, scale_results


from colour import RED, GREEN, YELLOW, C_NORMAL


def get_alphabet(texts):
    text = ''.join(''.join(x) for x in texts)
    a = charmodel.Alphabet(text, ignore_case=False, threshold=1e-9)
    print >>sys.stderr, "alphabet is %s" % (a.alphabet,)
    return a


def encode_and_cycle_texts(alphabet, texts, reverse=False):
    print >>sys.stderr, "remapping texts"
    if reverse:
        print >>sys.stderr, "... reversing texts"
        def f(x):
            return ''.join(reversed(x))
    else:
        def f(x):
            return x
    return {k: itertools.cycle(f(alphabet.encode_text(x)) for x in v)
            for k, v in texts.items()}


def get_net_and_corpus(srcdir, remap, accept_fn, control_corpus=None,
                       reverse=False, concat_texts=False,
                       **kwargs):
    raw_training_texts = load_texts(srcdir, remap, accept_fn,
                                    training_fn_search)
    if concat_texts:
        for v in raw_training_texts.values():
            v[:] = ['\n'.join(v)]

    raw_test_texts = load_texts(srcdir, remap, accept_fn,
                                test_fn_search)

    if control_corpus:
        raw_control_texts = load_texts(control_corpus, remap,
                                       always, always)
        raw_training_texts.update(raw_control_texts)

    alphabet = get_alphabet(raw_training_texts.values() +
                            raw_test_texts.values())
    textnames = sorted(raw_training_texts.keys())

    metadata = json.dumps({
        'alphabet': alphabet.alphabet,
        'collapse_chars': alphabet.collapsed_chars,
        'version': 1,
        'classnames': textnames,
        'case_insensitive': False,
        'utf8': True,
        'collapse_space': False,
    }, sort_keys=True)

    net = charmodel.Net(alphabet, textnames, metadata=metadata, **kwargs)
    training_texts = encode_and_cycle_texts(alphabet, raw_training_texts,
                                            reverse=reverse)
    test_texts = encode_and_cycle_texts(alphabet, raw_test_texts,
                                        reverse=reverse)
    return net, training_texts, test_texts


def maybe_swap_texts(training_texts, test_texts, threshold=0.9):
    for k, test_cycle in test_texts.items():
        training_cycle = training_texts[k]
        a = training_cycle.next()
        b = training_cycle.next()
        if a is not b:
            continue
        test_text = test_cycle.next()
        if len(a) < len(test_text) * threshold:
            print >> sys.stderr, ("swapping texts for %s: "
                                  "(test %d, training %d bytes)" %
                                  (k, len(test_text), len(a)))
            test_texts[k] = training_cycle
            training_texts[k] = test_cycle


def get_entropies_matrix(net, ignore_start, test_texts):
    py_entropies = []
    lengths = []
    for name in net.class_names:
        if name not in test_texts:
            continue
        cycle = test_texts.get(name)
        text = cycle.next()
        lengths.append(len(text))
        e = net.test(text, ignore_start, 1)
        py_entropies.append(e)
    return np.array(py_entropies)


def get_indices(net, ignore_start, test_texts):
    entropies = get_entropies_matrix(net, ignore_start, test_texts)
    rel_entropies = entropies - np.median(entropies, axis=0)

    norm = np.linalg.norm(rel_entropies, ord=1, axis=1)
    if np.all(norm):
        rel_entropies /= norm[:, None]

    class_names = net.class_names

    if 'control' in class_names:
        control_index = net.class_names.index('control')
        print >> sys.stderr, "deleting control %d" % control_index
        rel_entropies = np.delete(rel_entropies,
                                  control_index, axis=1)
        class_names = [x for x in class_names if x != 'control']

    self_scores = rel_entropies.diagonal()

    indices = [sorted(m).index(s) for s, m
               in zip(self_scores, rel_entropies)]

    return dict(zip(class_names, indices))


def write_pan_results(net, filename, ignore_start, test_texts,
                      hedge=0.0, raw=False):
    index_results = get_indices(net, ignore_start, test_texts)
    results = scale_results(index_results)
    if not raw:
        results = balance_results(results, cat1_radius=hedge)

    write_answers(filename, results)


def no_answer(*args):
    pass


def test(net, ignore_start, test_texts, ground_truth_oracle=no_answer):
    entropies = get_entropies_matrix(net, ignore_start, test_texts)
    rel_entropies = entropies - entropies.mean(axis=0)
    #rel_entropies = entropies - np.median(entropies, axis=0)

    lengths = [len(test_texts.get(name).next())
               for name in net.class_names
               if name in test_texts]

    if True:
        norm = np.linalg.norm(rel_entropies, ord=1, axis=1)
        if np.all(norm):
            rel_entropies /= norm[:, None]

    medians = np.median(rel_entropies, axis=1)

    try:
        control_index = net.class_names.index('control')
        self_scores = np.delete(rel_entropies,
                                control_index, axis=1).diagonal()
    except ValueError:
        self_scores = rel_entropies.diagonal()

    indices = [sorted(m).index(s) for s, m
               in zip(self_scores, rel_entropies)]
    #print indices
    #print self_scores
    sum_scores = [0, 0]
    sum_index = [0, 0]
    sum_diff = [0, 0]
    n_true, n_false = 0., 0.
    index_counts = [[0] * net.n_classes, [0] * net.n_classes]
    for name, score, index, median, length in zip(net.class_names, self_scores,
                                                  indices, medians, lengths):
        t = ground_truth_oracle(name)
        if t is None:
            print "%s as no truth!" % name
        print ("test: %s own % .3f median % .3f diff % .3f index %2d %5s %5db" %
               (name, score, median, median - score, index, t, length))
        sum_scores[t] += score
        sum_index[t] += index
        sum_diff[t] += median - score
        n_true += t
        n_false += not t
        index_counts[t][index] += 1

    print "scores:  N % .3f,  Y % .3f" % tuple(x / n_true
                                               for x in sum_scores)
    print "diffs:   N % .3f,  Y % .3f" % tuple(x / n_true
                                               for x in sum_diff)
    print "indices: N % .3f,  Y % .3f" % tuple(x / n_false
                                               for x in sum_index)

    t_colours = ([GREEN] * (net.n_classes / 4) +
                 [YELLOW] * (net.n_classes / 3) +
                 [RED] * (net.n_classes / 2))
    f_colours = ([RED] * (net.n_classes / 4) +
                 [YELLOW] * (net.n_classes / 3) +
                 [GREEN] * (net.n_classes / 2))

    sum_t, sum_f = 0, int(n_false)
    scale = 1.0 / (n_true + n_false)

    for i, tf in enumerate(zip(*index_counts)):
        f, t = tf
        s = t_colours[i]
        s += 'T' * t
        s += f_colours[i]
        s += 'F' * f
        sum_t += t
        sum_f -= f
        right = 't' * sum_t + 'f' * sum_f
        print "%.2f %30s %s%.2f %s" % (i * scale, s, C_NORMAL,
                                       float(sum_t + sum_f) / (n_true + n_false),
                                       right)
    print C_NORMAL


def train(net, texts, leakage, sub_epochs, leakage_decay, learn_rate_decay,
          ignore_start, test_texts=None, ground_truth_oracle=no_answer):

    validation_text = texts.get('control', texts.values()[0]).next()
    prev_validation_entropies = net.test(validation_text, ignore_start, 1)

    for i in range(sub_epochs):
        print ("doing sub-epoch %d with learn-rate %s, "
               "leakage %s" % (i, net.learn_rate, leakage))

        for name, cycle in texts.items():
            net.train(cycle.next(), name, leakage=leakage,
                      ignore_start=ignore_start)

        validation_entropies = net.test(validation_text, ignore_start, 1)
        ve_sum = 0.0
        ve_sum2 = 0.0
        ve_diff_sum = 0.0
        ve_diff_sum2 = 0.0
        for prev, e in zip(prev_validation_entropies, validation_entropies):
            diff = prev - e
            ve_diff_sum += diff
            ve_diff_sum2 += diff * diff
            ve_sum += e
            ve_sum2 += e * e
        ve_scale = 1.0 / len(validation_entropies)
        prev_validation_entropies = validation_entropies
        ve_mean = ve_sum * ve_scale
        ve_stddev = (ve_sum2 * ve_scale - ve_mean * ve_mean) ** 0.5
        ve_diff_mean = ve_diff_sum * ve_scale
        ve_diff_stddev = (ve_diff_sum2 * ve_scale -
                          ve_diff_mean * ve_diff_mean) ** 0.5

        print ("validation entropy %.3f±%.3f  diff % .3f±%.3f" %
               (ve_mean, ve_stddev, ve_diff_mean, ve_diff_stddev))

        if test_texts:
            test(net, ignore_start, test_texts, ground_truth_oracle)

        net.save()
        leakage *= leakage_decay
        net.learn_rate *= learn_rate_decay


def train_silently(net, texts, leakage, sub_epochs, leakage_decay,
                   learn_rate_decay, ignore_start, trace=None):

    for i in range(sub_epochs):
        for name, cycle in texts.items():
            net.train(cycle.next(), name, leakage=leakage,
                      ignore_start=ignore_start)
        leakage *= leakage_decay
        net.learn_rate *= learn_rate_decay
        if trace:
            trace(i)


def train_silently_balanced(net, texts, leakage, sub_epochs, leakage_decay,
                            learn_rate_decay, ignore_start, trace=None):
    from heapq import heappop, heappush

    control = texts.pop('control', None)
    training_counts = [(0, name, cycle) for name, cycle in texts.items()]
    n_classes = len(training_counts)

    for i in range(sub_epochs):
        if control is not None:
            net.train(control.next(), 'control', leakage=leakage,
                      ignore_start=ignore_start)
        epoch_len = 0
        epoch_texts = 0
        visited_classes = set()
        while len(visited_classes) < n_classes:
            n, name, cycle = heappop(training_counts)
            visited_classes.add(name)
            text = cycle.next()
            train_len = max(len(text) - ignore_start, 0)
            n += train_len
            epoch_len += train_len
            net.train(cycle.next(), name, leakage=leakage,
                      ignore_start=ignore_start)
            heappush(training_counts, (n, name, cycle))
            epoch_texts += 1

        print >> sys.stderr, ("EPOCH %d trained on %d chars in %d texts" %
                              (i, epoch_len, epoch_texts))
        leakage *= leakage_decay
        net.learn_rate *= learn_rate_decay
        if trace:
            trace(i)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('srcdir', help="find training text here")
    parser.add_argument('-n', '--basename',
                        help="base filenames upon this")
    parser.add_argument('-H', '--hidden-size', type=int, default=199,
                        metavar='<nodes>', help="number of hidden nodes")
    parser.add_argument('-r', '--rng-seed', type=int, default=-1,
                        help="rng seed (-1 for auto)")
    parser.add_argument('-e', '--sub-epochs', type=int, default=1,
                        help="how many cycles through the texts to do")
    parser.add_argument('--batch-size', type=int, default=20, metavar='<int>',
                        help="mini-batch size")
    parser.add_argument('--presynaptic-noise', type=float, default=0.1,
                        metavar='<float>',
                        help="Add this much presynaptic noise")
    parser.add_argument('-l', '--learn-rate', type=float, default=1e-1,
                        help=charmodel.Net.learn_rate.__doc__)
    parser.add_argument('-L', '--leakage', type=float, default=-1.0,
                        help=("how much training leaks into other classes "
                              "[0-1] or negative"))
    parser.add_argument('--leakage-decay', type=float, default=1.0,
                        help="change in leakage per sub-epoch")
    parser.add_argument('--learn-rate-decay', type=float, default=1,
                        help="change in learn-rate per sub-epoch")
    parser.add_argument('-m', '--momentum', type=float, default=0.95,
                        metavar='<0-1>', help=charmodel.Net.momentum.__doc__)
    parser.add_argument('--momentum-weight', type=float, default=0.5,
                        metavar='<0-1>',
                        help=charmodel.Net.momentum_weight.__doc__)
    parser.add_argument('--log-file', default=None,
                        help="log to this file")
    parser.add_argument('-v', '--verbose', action='store_true',
                        help="print more to stderr")
    parser.add_argument('--enable-fp-exceptions', action='store_true',
                        help="crash on bad floating point errors")
    parser.add_argument('--temporal-pgm-dump', action='store_true',
                        help=("save images showing changing state "
                              "of input/error vectors"))
    parser.add_argument('--periodic-pgm-dump',
                        metavar='"({ih,ho,bi}{w,m,d,t})*"',
                        help=("Periodically dump images of weights;"
                              "string determines which"))
    parser.add_argument('--periodic-pgm-period', type=int, default=10000,
                        help=("periodicity of periodic-pgm-dump"))
    parser.add_argument('--accept_re', metavar='REGEXP',
                        help="only use classes matching this pattern")
    parser.add_argument('--learning-method', type=int, default=4,
                        help=("0: weighted, 2: simplified N., "
                              "3: classical, 4: adagrad"))
    parser.add_argument('--activation', type=int, default=2,
                        help=("1: ReLU, 2: ReSQRT, 3: ReLOG, 4: "
                              "ReTANH, 5: clipped ReLU"))
    parser.add_argument('-d', '--bptt-depth', type=int, default=50,
                        help="how far to backpropagate through time")
    parser.add_argument('-i', '--ignore-start', type=int, default=0,
                        help="don't train on this many characters at start")
    parser.add_argument('-M', '--language-mapping',
                        help="use this character mapping")
    parser.add_argument('-f', '--filename',
                        help="save net here")
    parser.add_argument('--control-corpus',
                        help="add extra documents from here")
    parser.add_argument('--no-control-corpus', action='store_true',
                        help="don't use control corpus even if it is specified")
    parser.add_argument('--try-swapping-texts', action='store_true',
                        help="if known text is short, train on unknown text")
    parser.add_argument('--swap-corpora', action='store_true',
                        help="train with test text (implies --concat-texts)")
    parser.add_argument('--reverse', action='store_true',
                        help="learn and process all texts in reverse")
    parser.add_argument('--concat-texts', action='store_true',
                        help="treat training documents as one big document")
    parser.add_argument('--balanced-training', action='store_true',
                        help=("try to train each class equally, "
                              "regardless of text length"))
    parser.add_argument('--pan-answers',
                        help="write answers here in the style expected by PAN")
    parser.add_argument('--raw-answers',
                        help=("write answers here not adjusted for c@1 "
                              "(- for stdout)"))
    parser.add_argument('--raw-answers-trace',
                        help=("write raw answers after every epoch "
                              "(put %%d in name)"))
    parser.add_argument('--pan-hedge', type=float, default=0.0,
                        help="radius of undecided zone in pan output")
    parser.add_argument('--init-method', type=int, default=charmodel.INIT_FLAT,
                        help="0: zeros, 1: flat, 2: fan-in, 3: runs")

    args = parser.parse_args()

    if args.enable_fp_exceptions:
        charmodel.enable_fp_exceptions()

    if args.rng_seed != -1:
        random.seed(args.rng_seed)

    if args.accept_re:
        accept_fn = re.compile(args.accept_re).search
    else:
        accept_fn = always

    remap = mappings.get_charmap(args.language_mapping)

    net_kwargs = {}
    for k, v in vars(args).items():
        if k in ("bptt_depth",
                 "hidden_size",
                 "rng_seed",
                 "log_file",
                 "verbose",
                 "learn_rate",
                 "temporal_pgm_dump",
                 "periodic_pgm_dump",
                 "periodic_pgm_period",
                 "basename",
                 "activation",
                 "learning_method",
                 "batch_size",
                 "filename",
                 "presynaptic_noise",
                 "init_method",
        ):
            net_kwargs[k] = v

    concat_texts = args.concat_texts or args.swap_corpora

    if args.no_control_corpus:
        control_corpus = None
    else:
        control_corpus = args.control_corpus
    net, training_texts, test_texts = get_net_and_corpus(args.srcdir,
                                                         remap, accept_fn,
                                                         control_corpus,
                                                         args.reverse,
                                                         concat_texts,
                                                         **net_kwargs)
    if args.try_swapping_texts:
        maybe_swap_texts(training_texts, test_texts)

    if args.swap_corpora:
        training_texts, test_texts = test_texts, training_texts

    if args.raw_answers_trace is not None:
        def trace(i):
            fn = args.raw_answers_trace % i
            write_pan_results(net, fn, args.ignore_start,
                              test_texts, args.pan_hedge,
                              raw=True)
    else:
        trace = None

    if args.pan_answers or args.raw_answers:
        if args.balanced_training:
            train_silently_balanced(net, training_texts, args.leakage,
                                    args.sub_epochs, args.leakage_decay,
                                    args.learn_rate_decay, args.ignore_start,
                                    trace=trace)
        else:
            train_silently(net, training_texts, args.leakage,
                           args.sub_epochs, args.leakage_decay,
                           args.learn_rate_decay, args.ignore_start,
                           trace=trace)

        if args.pan_answers:
            write_pan_results(net, args.pan_answers, args.ignore_start,
                              test_texts, args.pan_hedge)
        if args.raw_answers:
            write_pan_results(net, args.raw_answers, args.ignore_start,
                              test_texts, args.pan_hedge, raw=True)

    else:
        ground_truth_oracle = read_truth_file(args.srcdir).get

        train(net, training_texts, args.leakage, args.sub_epochs,
              args.leakage_decay, args.learn_rate_decay, args.ignore_start,
              test_texts=test_texts, ground_truth_oracle=ground_truth_oracle)


main()
